{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBstvTA3yQJH0+gSa2g2OR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kim-Yeon-Jun/PyTorch/blob/main/pytorch_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 엘만 RNN\n",
        "RNNCell을 사용해 입력-은닉 가중치 행렬과 은닉-은닉 가중치 행렬을 만듦\\\n",
        "RNNCell() 호출마다 입력 벡터 행렬과 은닉 벡터 행렬을 받음\\\n",
        "그 다음 이 타임 스텝의 은닉 벡터 행렬과 결과를 반환함"
      ],
      "metadata": {
        "id": "gAfv4uW4xorA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XviYhJhVwecY"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class ElmanRNN(nn.Module):\n",
        "  \"\"\"RNNCell을 사용하여 만든 엘만 RNN\n",
        "    매개변수 :\n",
        "      input_size (int) : 입력 벡터 크기\n",
        "      hidden_size(int) : 은닉 상태 벡터 크기\n",
        "      batch_first (bool) : 0번째 차원이 배치인지 여부\n",
        "  \"\"\"\n",
        "  super(ElmanRNN, self).__init__()\n",
        "  self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
        "\n",
        "  self.batch_first = batch_first\n",
        "  self.hidden_size = hidden_size\n",
        "  def _initialize_hidden(self, batch_size):\n",
        "    return torch.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "  def forward(self, x_in, initial_hidden = None):\n",
        "    \"\"\"ElmanRNN의 정방향 계산\n",
        "\n",
        "      매개변수 :\n",
        "        x_in (torch.Tensor) : 입력 데이터 텐서\n",
        "          If self.btach_first : x_in.shape = (batch_size, seq_size, feat_size)\n",
        "          Else : x_in.shape = (seq_size, batch_size, feat_size)\n",
        "        initial_hidden (torch.Tensor) : RNN의 초기 은닉 상태\n",
        "\n",
        "      반환값 :\n",
        "        hiddens (torch.Tensor) : 각 타임 스텝에서 RNN 출력\n",
        "          If self.batch_first :\n",
        "            hiddens.shape = (batch_size, seq_size, hidden_size)\n",
        "          Else : hiddens.shape = (seq_size, batch_size, hidden_size)\n",
        "    \"\"\"\n",
        "\n",
        "    if self.batch_first:\n",
        "      batch_size, seq_size, feat_size = x_in.size()\n",
        "      x_in = x_in.permute(1,0,2)\n",
        "    else :\n",
        "      seq_size, batch_size, feat_size = x_in.size()\n",
        "\n",
        "    hiddens = []\n",
        "\n",
        "    if initial_hidden is None:\n",
        "      initial_hidden = self._initialize_hidden(batch_size)\n",
        "      initial_hidden = initial_hidden.to(x_in.device)\n",
        "\n",
        "    hidden_t = initial_hidden\n",
        "\n",
        "    for t in range(seq_size):\n",
        "      hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
        "      hiddens.append(hidden_t)\n",
        "\n",
        "    hiddens = torch.stack(hiddens)\n",
        "\n",
        "    if self.batch_first:\n",
        "      hiddens = hiddens.permute(1,0,2)\n",
        "\n",
        "    return hiddens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 : 문자 RNN으로 성씨 국적 분류하기\n"
      ],
      "metadata": {
        "id": "Jw5tFGUr0xyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SurnameDataset 클래스 구현\n",
        "class SurnameDataset(Dataset):\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
        "        \"\"\"데이터셋을 로드하고 새로운 Vectorizer 객체를 만듭니다\n",
        "\n",
        "        매개변수:\n",
        "            surname_csv (str): 데이터셋의 위치\n",
        "        반환값:\n",
        "            SurnameDataset의 객체\n",
        "        \"\"\"\n",
        "        surname_df = pd.read_csv(surname_csv)\n",
        "        train_surname_df = surname_df[surname_df.split=='train']\n",
        "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"파이토치 데이터셋의 주요 진입 메서드\n",
        "\n",
        "        매개변수:\n",
        "            index (int): 데이터 포인트 인덱스\n",
        "        반환값:\n",
        "            다음 값을 담고 있는 딕셔너리:\n",
        "                특성 (x_data)\n",
        "                레이블 (y_target)\n",
        "                특성 길이 (x_length)\n",
        "        \"\"\"\n",
        "        row = self._target_df.iloc[index]\n",
        "\n",
        "        surname_vector, vec_length = \\\n",
        "            self._vectorizer.vectorize(row.surname, self._max_seq_length)\n",
        "\n",
        "        nationality_index = \\\n",
        "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
        "\n",
        "        return {'x_data': surname_vector,\n",
        "                'y_target': nationality_index,\n",
        "                'x_length': vec_length}"
      ],
      "metadata": {
        "id": "0MX68QLg0yBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SurnameVectorizer 구현\n",
        "class SurnameVectorizer(object):\n",
        "    \"\"\" 어휘 사전을 생성하고 관리합니다 \"\"\"\n",
        "    def __init__(self, char_vocab, nationality_vocab):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            char_vocab (Vocabulary): 문자를 정수로 매핑합니다\n",
        "            nationality_vocab (Vocabulary): 국적을 정수로 매핑합니다\n",
        "        \"\"\"\n",
        "        self.char_vocab = char_vocab\n",
        "        self.nationality_vocab = nationality_vocab\n",
        "\n",
        "    def vectorize(self, surname, vector_length=-1):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            title (str): 문자열\n",
        "            vector_length (int): 인덱스 벡터의 길이를 맞추기 위한 매개변수\n",
        "        \"\"\"\n",
        "        indices = [self.char_vocab.begin_seq_index]\n",
        "        indices.extend(self.char_vocab.lookup_token(token)\n",
        "                       for token in surname)\n",
        "        indices.append(self.char_vocab.end_seq_index)\n",
        "\n",
        "        if vector_length < 0:\n",
        "            vector_length = len(indices)\n",
        "\n",
        "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
        "        out_vector[:len(indices)] = indices\n",
        "        out_vector[len(indices):] = self.char_vocab.mask_index\n",
        "\n",
        "        return out_vector, len(indices)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, surname_df):\n",
        "        \"\"\"데이터셋 데이터프레임으로 SurnameVectorizer 객체를 초기화합니다.\n",
        "\n",
        "        매개변수:\n",
        "            surname_df (pandas.DataFrame): 성씨 데이터셋\n",
        "        반환값:\n",
        "            SurnameVectorizer 객체\n",
        "        \"\"\"\n",
        "        char_vocab = SequenceVocabulary()\n",
        "        nationality_vocab = Vocabulary()\n",
        "\n",
        "        for index, row in surname_df.iterrows():\n",
        "            for char in row.surname:\n",
        "                char_vocab.add_token(char)\n",
        "            nationality_vocab.add_token(row.nationality)\n",
        "\n",
        "        return cls(char_vocab, nationality_vocab)"
      ],
      "metadata": {
        "id": "YRmZnzM40yf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Elamn RNN을 사용한 SurnameClassifier 모델 구현\n",
        "class SurnameClassifier(nn.Module):\n",
        "    \"\"\" RNN으로 특성을 추출하고 MLP로 분류하는 분류 모델 \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, num_classes,\n",
        "                 rnn_hidden_size, batch_first=True, padding_idx=0):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            embedding_size (int): 문자 임베딩의 크기\n",
        "            num_embeddings (int): 임베딩할 문자 개수\n",
        "            num_classes (int): 예측 벡터의 크기\n",
        "                노트: 국적 개수\n",
        "            rnn_hidden_size (int): RNN의 은닉 상태 크기\n",
        "            batch_first (bool): 입력 텐서의 0번째 차원이 배치인지 시퀀스인지 나타내는 플래그\n",
        "            padding_idx (int): 텐서 패딩을 위한 인덱스;\n",
        "                torch.nn.Embedding을 참고하세요\n",
        "        \"\"\"\n",
        "        super(SurnameClassifier, self).__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
        "                                embedding_dim=embedding_size,\n",
        "                                padding_idx=padding_idx)\n",
        "        self.rnn = ElmanRNN(input_size=embedding_size,\n",
        "                             hidden_size=rnn_hidden_size,\n",
        "                             batch_first=batch_first)\n",
        "        self.fc1 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                         out_features=rnn_hidden_size)\n",
        "        self.fc2 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                          out_features=num_classes)\n",
        "\n",
        "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
        "        \"\"\" 분류기의 정방향 계산\n",
        "\n",
        "        매개변수:\n",
        "            x_in (torch.Tensor): 입력 데이터 텐서\n",
        "                x_in.shape는 (batch, input_dim)입니다\n",
        "            x_lengths (torch.Tensor): 배치에 있는 각 시퀀스의 길이\n",
        "                시퀀스의 마지막 벡터를 찾는데 사용합니다\n",
        "            apply_softmax (bool): 소프트맥스 활성화 함수를 위한 플래그\n",
        "                크로스-엔트로피 손실을 사용하려면 False로 지정합니다\n",
        "        반환값:\n",
        "            결과 텐서. tensor.shape는 (batch, output_dim)입니다.\n",
        "        \"\"\"\n",
        "        x_embedded = self.emb(x_in)\n",
        "        y_out = self.rnn(x_embedded)\n",
        "\n",
        "        if x_lengths is not None:\n",
        "            y_out = column_gather(y_out, x_lengths)\n",
        "        else:\n",
        "            y_out = y_out[:, -1, :]\n",
        "\n",
        "        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))\n",
        "        y_out = self.fc2(F.dropout(y_out, 0.5))\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "\n",
        "        return y_out"
      ],
      "metadata": {
        "id": "0ROAjtws0yii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = Namespace(\n",
        "    # 날짜와 경로 정보\n",
        "    surname_csv=\"data/surnames/surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"model_storage/ch6/surname_classification\",\n",
        "    # 모델 하이퍼파라미터\n",
        "    char_embedding_size=100,\n",
        "    rnn_hidden_size=64,\n",
        "    # 훈련 하이퍼파라미터\n",
        "    num_epochs=100,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    seed=1337,\n",
        "    early_stopping_criteria=5,\n",
        "    # 실행 옵션\n",
        "    cuda=True,\n",
        "    catch_keyboard_interrupt=True,\n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        ")"
      ],
      "metadata": {
        "id": "eHXcSEwi0yki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yub-DjaL0yni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zfAvPjqm0ypa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}