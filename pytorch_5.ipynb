{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhNWyy15fPY4ioLKW1cxL0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kim-Yeon-Jun/PyTorch/blob/main/pytorch_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade annoy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYtYvYcbmRrW",
        "outputId": "329e3f53-5565-4500-9b8b-effa9abfbfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: annoy in /usr/local/lib/python3.10/dist-packages (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NsNf5JHhoMn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from annoy import AnnoyIndex\n",
        "import numpy as np\n",
        "class PreTrainedEmbeddings(object):\n",
        "    \"\"\" 사전 훈련된 단어 벡터 사용을 위한 래퍼 클래스 \"\"\"\n",
        "    def __init__(self, word_to_index, word_vectors):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            word_to_index (dict): 단어에서 정수로 매핑\n",
        "            word_vectors (numpy 배열의 리스트)\n",
        "        \"\"\"\n",
        "        self.word_to_index = word_to_index\n",
        "        self.word_vectors = word_vectors\n",
        "        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n",
        "\n",
        "        self.index = AnnoyIndex(len(word_vectors[0]), metric='euclidean')\n",
        "        print(\"인덱스 만드는 중!\")\n",
        "        for _, i in self.word_to_index.items():\n",
        "            self.index.add_item(i, self.word_vectors[i])\n",
        "        self.index.build(50)\n",
        "        print(\"완료!\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_embeddings_file(cls, embedding_file):\n",
        "        \"\"\"사전 훈련된 벡터 파일에서 객체를 만듭니다.\n",
        "\n",
        "        벡터 파일은 다음과 같은 포맷입니다:\n",
        "            word0 x0_0 x0_1 x0_2 x0_3 ... x0_N\n",
        "            word1 x1_0 x1_1 x1_2 x1_3 ... x1_N\n",
        "\n",
        "        매개변수:\n",
        "            embedding_file (str): 파일 위치\n",
        "        반환값:\n",
        "            PretrainedEmbeddings의 인스턴스\n",
        "        \"\"\"\n",
        "        word_to_index = {}\n",
        "        word_vectors = []\n",
        "\n",
        "        with open(embedding_file) as fp:\n",
        "            for line in fp.readlines():\n",
        "                line = line.split(\" \")\n",
        "                word = line[0]\n",
        "                vec = np.array([float(x) for x in line[1:]])\n",
        "\n",
        "                word_to_index[word] = len(word_to_index)\n",
        "                word_vectors.append(vec)\n",
        "\n",
        "        return cls(word_to_index, word_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 임베딩을 사용한 유추 작업\n",
        "import numpy as np\n",
        "from annoy import AnnoyIndex\n",
        "\n",
        "class PreTrainedEmbeddings(object):\n",
        "    def __init__(self, word_to_index, word_vectors):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            word_to_index (dict): 단어에서 정수로 매핑\n",
        "            word_vectors (numpy 배열의 리스트)\n",
        "        \"\"\"\n",
        "        self.word_to_index = word_to_index\n",
        "        self.word_vectors = word_vectors\n",
        "        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n",
        "\n",
        "        self.index = AnnoyIndex(len(word_vectors[0]), metric='euclidean')\n",
        "        print(\"인덱스 만드는 중!\")\n",
        "        for _, i in self.word_to_index.items():\n",
        "            self.index.add_item(i, self.word_vectors[i])\n",
        "        self.index.build(50)\n",
        "        print(\"완료!\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_embeddings_file(cls, embedding_file):\n",
        "        \"\"\"사전 훈련된 벡터 파일에서 객체를 만듭니다.\n",
        "\n",
        "        벡터 파일은 다음과 같은 포맷입니다:\n",
        "            word0 x0_0 x0_1 x0_2 x0_3 ... x0_N\n",
        "            word1 x1_0 x1_1 x1_2 x1_3 ... x1_N\n",
        "\n",
        "        매개변수:\n",
        "            embedding_file (str): 파일 위치\n",
        "        반환값:\n",
        "            PretrainedEmbeddings의 인스턴스\n",
        "        \"\"\"\n",
        "        word_to_index = {}\n",
        "        word_vectors = []\n",
        "\n",
        "        with open(embedding_file) as fp:\n",
        "            for line in fp.readlines():\n",
        "                line = line.split(\" \")\n",
        "                word = line[0]\n",
        "                vec = np.array([float(x) for x in line[1:]])\n",
        "\n",
        "                word_to_index[word] = len(word_to_index)\n",
        "                word_vectors.append(vec)\n",
        "    def get_embedding(self, word):\n",
        "      \"\"\"\n",
        "          매개변수 :\n",
        "            word(str)\n",
        "          반환값 :\n",
        "            임베딩 (numpy.ndarray)\n",
        "      \"\"\"\n",
        "      return self.word_vectors[self.word_to_index[word]]\n",
        "\n",
        "    def get_closest_to_vector(self, vector, n=1):\n",
        "      \"\"\" 벡터가 주어지면 최근접 이웃을 n개 반환\n",
        "      매개변수 :\n",
        "        vector(np.ndarray) : Annoy 인덱스에 있는 벡터의 크기와 같아야 함\n",
        "        n (int) : 반환될 이웃의 개수\n",
        "        반환값 :\n",
        "          [str, str, str ...] : 주어진 벡터와 가장 가까운 단어\n",
        "              단어는 거리순으로 정렬되어있는 것은 아님.\n",
        "\n",
        "      \"\"\"\n",
        "      nn_indices = self.index_get_nns_by_vector(vector, n)\n",
        "      return [self.index_to_word[neighbor]\n",
        "              for neighbor in nn_indices]\n",
        "\n",
        "    def compute_and_print_analogy(self, word1, word2, word3):\n",
        "      \"\"\" 단어 임베딩을 사용한 유추 결과를 출력\n",
        "\n",
        "      word1이 word2 일 때 word3은 __입니다.\n",
        "      이 메서드는 word1 : word2 :: word3 : word4 를 출력함\n",
        "\n",
        "      매개변수 :\n",
        "        word1(str)\n",
        "        word2(str)\n",
        "        word3(str)\n",
        "      \"\"\"\n",
        "      vec1 = self.get_embedding(word1)\n",
        "      vec2 = self.get_embedding(word2)\n",
        "      vec3 = self.get_embedding(word3)\n",
        "\n",
        "      #단순 가정 : 이 유추는 공간적 관계\n",
        "      spatial_relationship = vec2 - vec1\n",
        "      vec4 = vec3 + spatial_relationship\n",
        "\n",
        "      closest_words = self.get_closest_to_vector(vec4, n=4)\n",
        "      existing_words = set([word1,word2,word3])\n",
        "      closest_words = [word for word in closest_words\n",
        "                       if word not in existing_words]\n",
        "      if len(closest_words) == 0 :\n",
        "        print(\"계산된 벡터와 가장 가까운 이웃을 찾을 수 없습니다.\")\n",
        "        return\n",
        "      for word4 in closest_words:\n",
        "        print(\"{} : {} :: {} : {}\".format(word1, word2, word3, word4))\n",
        "\n"
      ],
      "metadata": {
        "id": "gSeRUzbSksp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 : CBOW 임베딩 학습"
      ],
      "metadata": {
        "id": "8T3I0yPB5Puo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\" 매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스 \"\"\"\n",
        "\n",
        "    def __init__(self, token_to_idx=None, mask_token=\"<MASK>\", add_unk=True, unk_token=\"<UNK>\"):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            token_to_idx (dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
        "            mask_token (str): Vocabulary에 추가할 MASK 토큰.\n",
        "                모델 파라미터를 업데이트하는데 사용하지 않는 위치를 나타냅니다.\n",
        "            add_unk (bool): UNK 토큰을 추가할지 지정하는 플래그\n",
        "            unk_token (str): Vocabulary에 추가할 UNK 토큰\n",
        "        \"\"\"\n",
        "\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self._token_to_idx = token_to_idx\n",
        "\n",
        "        self._idx_to_token = {idx: token\n",
        "                              for token, idx in self._token_to_idx.items()}\n",
        "\n",
        "        self._add_unk = add_unk\n",
        "        self._unk_token = unk_token\n",
        "        self._mask_token = mask_token\n",
        "\n",
        "        self.mask_index = self.add_token(self._mask_token)\n",
        "        self.unk_index = -1\n",
        "        if add_unk:\n",
        "            self.unk_index = self.add_token(unk_token)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        \"\"\" 직렬화할 수 있는 딕셔너리를 반환합니다 \"\"\"\n",
        "        return {'token_to_idx': self._token_to_idx,\n",
        "                'add_unk': self._add_unk,\n",
        "                'unk_token': self._unk_token,\n",
        "                'mask_token': self._mask_token}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        \"\"\" 직렬화된 딕셔너리에서 Vocabulary 객체를 만듭니다 \"\"\"\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        \"\"\" 토큰을 기반으로 매핑 딕셔너리를 업데이트합니다\n",
        "\n",
        "        매개변수:\n",
        "            token (str): Vocabulary에 추가할 토큰\n",
        "        반환값:\n",
        "            index (int): 토큰에 상응하는 정수\n",
        "        \"\"\"\n",
        "        if token in self._token_to_idx:\n",
        "            index = self._token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self._token_to_idx)\n",
        "            self._token_to_idx[token] = index\n",
        "            self._idx_to_token[index] = token\n",
        "        return index\n",
        "\n",
        "    def add_many(self, tokens):\n",
        "        \"\"\" 토큰 리스트를 Vocabulary에 추가합니다.\n",
        "\n",
        "        매개변수:\n",
        "            tokens (list): 문자열 토큰 리스트\n",
        "        반환값:\n",
        "            indices (list): 토큰 리스트에 상응되는 인덱스 리스트\n",
        "        \"\"\"\n",
        "        return [self.add_token(token) for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\" 토큰에 대응하는 인덱스를 추출합니다.\n",
        "        토큰이 없으면 UNK 인덱스를 반환합니다.\n",
        "\n",
        "        매개변수:\n",
        "            token (str): 찾을 토큰\n",
        "        반환값:\n",
        "            index (int): 토큰에 해당하는 인덱스\n",
        "        노트:\n",
        "            UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해)\n",
        "            `unk_index`가 0보다 커야 합니다.\n",
        "        \"\"\"\n",
        "        if self.unk_index >= 0:\n",
        "            return self._token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            return self._token_to_idx[token]\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        \"\"\" 인덱스에 해당하는 토큰을 반환합니다.\n",
        "\n",
        "        매개변수:\n",
        "            index (int): 찾을 인덱스\n",
        "        반환값:\n",
        "            token (str): 인텍스에 해당하는 토큰\n",
        "        에러:\n",
        "            KeyError: 인덱스가 Vocabulary에 없을 때 발생합니다.\n",
        "        \"\"\"\n",
        "        if index not in self._idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self._idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._token_to_idx)"
      ],
      "metadata": {
        "id": "TYygUSmz3kLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOWVectorizer(object):\n",
        "    \"\"\" 어휘 사전을 생성하고 관리합니다 \"\"\"\n",
        "    def __init__(self, cbow_vocab):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            cbow_vocab (Vocabulary): 단어를 정수에 매핑합니다\n",
        "        \"\"\"\n",
        "        self.cbow_vocab = cbow_vocab\n",
        "\n",
        "    def vectorize(self, context, vector_length=-1):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            context (str): 공백으로 나누어진 단어 문자열\n",
        "            vector_length (int): 인덱스 벡터의 길이 매개변수\n",
        "        \"\"\"\n",
        "\n",
        "        indices = [self.cbow_vocab.lookup_token(token) for token in context.split(' ')]\n",
        "        if vector_length < 0:\n",
        "            vector_length = len(indices)\n",
        "\n",
        "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
        "        out_vector[:len(indices)] = indices\n",
        "        out_vector[len(indices):] = self.cbow_vocab.mask_index\n",
        "\n",
        "        return out_vector\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, cbow_df):\n",
        "        \"\"\"데이터셋 데이터프레임에서 Vectorizer 객체를 만듭니다\n",
        "\n",
        "        매개변수::\n",
        "            cbow_df (pandas.DataFrame): 타깃 데이터셋\n",
        "        반환값:\n",
        "            CBOWVectorizer 객체\n",
        "        \"\"\"\n",
        "        cbow_vocab = Vocabulary()\n",
        "        for index, row in cbow_df.iterrows():\n",
        "            for token in row.context.split(' '):\n",
        "                cbow_vocab.add_token(token)\n",
        "            cbow_vocab.add_token(row.target)\n",
        "\n",
        "        return cls(cbow_vocab)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        cbow_vocab = \\\n",
        "            Vocabulary.from_serializable(contents['cbow_vocab'])\n",
        "        return cls(cbow_vocab=cbow_vocab)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'cbow_vocab': self.cbow_vocab.to_serializable()}"
      ],
      "metadata": {
        "id": "XQB7ixN33d9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CBOW 작업을 위한 데이터셋 클래스\n",
        "class CBOWDataset(Dataset):\n",
        "    def __init__(self, cbow_df, vectorizer):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            cbow_df (pandas.DataFrame): 데이터셋\n",
        "            vectorizer (CBOWVectorizer): 데이터셋에서 만든 CBOWVectorizer 객체\n",
        "        \"\"\"\n",
        "        self.cbow_df = cbow_df\n",
        "        self._vectorizer = vectorizer\n",
        "\n",
        "        measure_len = lambda context: len(context.split(\" \"))\n",
        "        self._max_seq_length = max(map(measure_len, cbow_df.context))\n",
        "\n",
        "        self.train_df = self.cbow_df[self.cbow_df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "\n",
        "        self.val_df = self.cbow_df[self.cbow_df.split=='val']\n",
        "        self.validation_size = len(self.val_df)\n",
        "\n",
        "        self.test_df = self.cbow_df[self.cbow_df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "\n",
        "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
        "                             'val': (self.val_df, self.validation_size),\n",
        "                             'test': (self.test_df, self.test_size)}\n",
        "\n",
        "        self.set_split('train')\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, cbow_csv):\n",
        "        \"\"\"데이터셋을 로드하고 처음부터 새로운 Vectorizer 만들기\n",
        "\n",
        "        매개변수:\n",
        "            cbow_csv (str): 데이터셋의 위치\n",
        "        반환값:\n",
        "            CBOWDataset의 인스턴스\n",
        "        \"\"\"\n",
        "        cbow_df = pd.read_csv(cbow_csv)\n",
        "        train_cbow_df = cbow_df[cbow_df.split=='train']\n",
        "        return cls(cbow_df, CBOWVectorizer.from_dataframe(train_cbow_df))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"파이토치 데이터셋의 주요 진입 메서드\n",
        "\n",
        "        매개변수:\n",
        "            index (int): 데이터 포인트의 인덱스\n",
        "        반환값:\n",
        "            데이터 포인트의 특성(x_data)과 레이블(y_target)로 이루어진 딕셔너리\n",
        "        \"\"\"\n",
        "        row = self._target_df.iloc[index]\n",
        "\n",
        "        context_vector = \\\n",
        "            self._vectorizer.vectorize(row.context, self._max_seq_length)\n",
        "        target_index = self._vectorizer.cbow_vocab.lookup_token(row.target)\n",
        "\n",
        "        return {'x_data': context_vector,\n",
        "                'y_target': target_index}"
      ],
      "metadata": {
        "id": "7_z0PztC1YIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOWClassifier(nn.Module): # Simplified cbow Model\n",
        "    def __init__(self, vocabulary_size, embedding_size, padding_idx=0):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            vocabulary_size (int): 어휘 사전 크기, 임베딩 개수와 예측 벡터 크기를 결정합니다\n",
        "            embedding_size (int): 임베딩 크기\n",
        "            padding_idx (int): 기본값 0; 임베딩은 이 인덱스를 사용하지 않습니다\n",
        "        \"\"\"\n",
        "        super(CBOWClassifier, self).__init__()\n",
        "\n",
        "        self.embedding =  nn.Embedding(num_embeddings=vocabulary_size,\n",
        "                                       embedding_dim=embedding_size,\n",
        "                                       padding_idx=padding_idx)\n",
        "        self.fc1 = nn.Linear(in_features=embedding_size,\n",
        "                             out_features=vocabulary_size)\n",
        "\n",
        "    def forward(self, x_in, apply_softmax=False):\n",
        "        \"\"\"분류기의 정방향 계산\n",
        "\n",
        "        매개변수:\n",
        "            x_in (torch.Tensor): 입력 데이터 텐서\n",
        "                x_in.shape는 (batch, input_dim)입니다.\n",
        "            apply_softmax (bool): 소프트맥스 활성화 함수를 위한 플래그\n",
        "                크로스-엔트로피 손실을 사용하려면 False로 지정합니다\n",
        "        반환값:\n",
        "            결과 텐서. tensor.shape은 (batch, output_dim)입니다.\n",
        "        \"\"\"\n",
        "        x_embedded_sum = F.dropout(self.embedding(x_in).sum(dim=1), 0.3)\n",
        "        y_out = self.fc1(x_embedded_sum)\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "\n",
        "        return y_out"
      ],
      "metadata": {
        "id": "0xEk9icA35Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CBOW 훈련 매개변수\n",
        "from argparse import Namespace\n",
        "args = Namespace(\n",
        "    # 날짜와 경로 정보\n",
        "    cbow_csv=\"data/books/frankenstein_with_splits.csv\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"model_storage/ch5/cbow\",\n",
        "    # 모델 하이퍼파라미터\n",
        "    embedding_size=50,\n",
        "    # 훈련 하이퍼파라미터\n",
        "    seed=1337,\n",
        "    num_epochs=100,\n",
        "    learning_rate=0.0001,\n",
        "    batch_size=32,\n",
        "    early_stopping_criteria=5,\n",
        "    # 실행 옵션\n",
        "    cuda=True,\n",
        "    catch_keyboard_interrupt=True,\n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True\n",
        ")"
      ],
      "metadata": {
        "id": "xlpjNirM4TRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 : 문서 분류에 사전 훈련된 임베딩을 사용한 전이학습\n",
        "###### 전이학습 : 한 작업에서 훈련된 모델을 다른 작업의 초기 모델로 사용하는 방식"
      ],
      "metadata": {
        "id": "1zJ_gOMB5Tk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NewsDataset.__getitem__() 메서드\n",
        "class NewDataset(Dataset):\n",
        "  @classmethod\n",
        "  def load_dataset_and_make_vectorizer(cls, news_csv):\n",
        "    \"\"\"데이터셋을 로드하고 처음부터 새로운 Vectorizer 만들기\n",
        "      매개변수 :\n",
        "        news_csv (str) : 데이터셋의 위치\n",
        "      반환값 :\n",
        "        NewsDataset의 인스턴스\n",
        "    \"\"\"\n",
        "    news_df = pd.read_csv(news_csv)\n",
        "    train_news_df = news_df[news_df.split == 'train']\n",
        "    return cls(news_df, NewsVectorizer.from_dataframe(train_news_df))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"파이토치 데이터셋의 주요 진입 메서드\n",
        "      매개변수 :\n",
        "        index(int) : 데이터 포인트의 인덱스\n",
        "\n",
        "      반환값 :\n",
        "        데이터 포인트의 특성(x_data)과 레이블(y_target)로 이루어진 딕셔너리\n",
        "    \"\"\"\n",
        "    row = self.target_df.iloc[index]\n",
        "\n",
        "    title_vector = \\\n",
        "      self._vectorizer.vectorize(row.title, self._max_seq_length)\n",
        "\n",
        "    category_index = \\\n",
        "      self._vectorizer.category_vocab.lookup_token(row.category)\n",
        "\n",
        "    return {'x_data' : title_vector,\n",
        "            'y_target' : category_index}"
      ],
      "metadata": {
        "id": "iTykbRjA4TfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SequenceVocabulary\n",
        "UNK 토큰 : 모델이 드물게 등장하는 단어에 대한 표현을 학습하도록 함\\\n",
        "MASK 토큰 : Embedding 층의 마스킹 역할을 수행하고 가변 길이의 시퀀스가 있을 때 손실계산을 도움\\\n",
        "BEGIN-OF-SEQUENCE와 END-OF-SEQUENCE 토큰 : 시퀀스 경계에 관한 힌트를 신경망에 제공"
      ],
      "metadata": {
        "id": "k_JdmhJq-GXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AG뉴스 데이터셋을 위한 Vectorizer 구현\n",
        "class NewsVectorizer(object):\n",
        "  def vectorize(self, title, vector_length=-1):\n",
        "    \"\"\"\n",
        "      매개변수 :\n",
        "        title(str) : 공백으로 나누어진 단어 문자열\n",
        "        vector_length(int) : 인덱스 벡터의 길이 매개변수\n",
        "\n",
        "      반환값 :\n",
        "        벡터로 변환된 제목(numpy.array)\n",
        "    \"\"\"\n",
        "    indices = [self.title_vocab.begin_seq_index]\n",
        "    indices.extend(self.title_vocab.lookup_token(token)\n",
        "    for token in title.split(\" \"))\n",
        "    indices.append(self.title_vocab.end_seq_index)\n",
        "    if vector_length < 0 :\n",
        "      vector_length = len(indices)\n",
        "    out_vector = np.zeros(vector_length, dtype=np.int64)\n",
        "    out_vector[:len(indices) = indices]\n",
        "    out_vector[len(indices):] = self.title_vocab.mask_index\n",
        "\n",
        "    return out_vector\n",
        "\n",
        "  @classmethod\n",
        "  def from_dataframe(cls, news_df, cutoff=25):\n",
        "    \"\"\" 데이터셋 데이터프레임에서 Vectorizer 객체를 만듦\n",
        "\n",
        "      매개변수 :\n",
        "        news_df (pandas.DataFrame) : 타깃 데이터셋\n",
        "        cutoff(int) : Vocabulary에 포함할 빈도 임계값\n",
        "      반환값 :\n",
        "        NewsVectorizer 객체\n",
        "    \"\"\"\n",
        "    category_vocab = Vocabulary()\n",
        "    for category in sorted(set(news_df.category)):\n",
        "      category_vocab.add_token(category)\n",
        "      word_counts = Counter()\n",
        "    for title in news_df.title:\n",
        "      for token in title.split(\" \"):\n",
        "        word_coutns[token] += 1\n",
        "    title_vocab = SequenceVocabulary()\n",
        "    for word, word_count in word_counts.items():\n",
        "      if word_count >= cutoff:\n",
        "        title_vocab.add_token(word)\n",
        "    return cls(title_vocab, category_vocab)"
      ],
      "metadata": {
        "id": "AOv48GnP4TkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#어휘 사전에 기반하여 단어 임베딩의 부분 집합을 선택함\n",
        "def load_glove_from_file(glove_filepath):\n",
        "    \"\"\"GloVe 임베딩 로드\n",
        "\n",
        "    매개변수:\n",
        "        glove_filepath (str): 임베딩 파일 경로\n",
        "    반환값:\n",
        "        word_to_index (dict), embeddings (numpy.ndarary)\n",
        "    \"\"\"\n",
        "\n",
        "    word_to_index = {}\n",
        "    embeddings = []\n",
        "    with open(glove_filepath, \"r\") as fp:\n",
        "        for index, line in enumerate(fp):\n",
        "            line = line.split(\" \") # each line: word num1 num2 ...\n",
        "            word_to_index[line[0]] = index # word = line[0]\n",
        "            embedding_i = np.array([float(val) for val in line[1:]])\n",
        "            embeddings.append(embedding_i)\n",
        "    return word_to_index, np.stack(embeddings)\n",
        "\n",
        "def make_embedding_matrix(glove_filepath, words):\n",
        "    \"\"\"\n",
        "    특정 단어 집합에 대한 임베딩 행렬을 만듭니다.\n",
        "\n",
        "    매개변수:\n",
        "        glove_filepath (str): 임베딩 파일 경로\n",
        "        words (list): 단어 리스트\n",
        "    \"\"\"\n",
        "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
        "    embedding_size = glove_embeddings.shape[1]\n",
        "\n",
        "    final_embeddings = np.zeros((len(words), embedding_size))\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if word in word_to_idx:\n",
        "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
        "        else:\n",
        "            embedding_i = torch.ones(1, embedding_size)\n",
        "            torch.nn.init.xavier_uniform_(embedding_i)\n",
        "            final_embeddings[i, :] = embedding_i\n",
        "\n",
        "    return final_embeddings"
      ],
      "metadata": {
        "id": "SYJbRk1SB-rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsClassifier(nn.Module):\n",
        "    def __init__(self, embedding_size, num_embeddings, num_channels,\n",
        "                 hidden_dim, num_classes, dropout_p,\n",
        "                 pretrained_embeddings=None, padding_idx=0):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            embedding_size (int): 임베딩 벡터의 크기\n",
        "            num_embeddings (int): 임베딩 벡터의 개수\n",
        "            num_channels (int): 합성곱 커널 개수\n",
        "            hidden_dim (int): 은닉 차원 크기\n",
        "            num_classes (int): 클래스 개수\n",
        "            dropout_p (float): 드롭아웃 확률\n",
        "            pretrained_embeddings (numpy.array): 사전에 훈련된 단어 임베딩\n",
        "                기본값은 None\n",
        "            padding_idx (int): 패딩 인덱스\n",
        "        \"\"\"\n",
        "        super(NewsClassifier, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)\n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "\n",
        "\n",
        "        self.convnet = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=embedding_size,\n",
        "                   out_channels=num_channels, kernel_size=3),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels,\n",
        "                   kernel_size=3, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels,\n",
        "                   kernel_size=3, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels,\n",
        "                   kernel_size=3),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "        self._dropout_p = dropout_p\n",
        "        self.fc1 = nn.Linear(num_channels, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x_in, apply_softmax=False):\n",
        "        \"\"\"분류기의 정방향 계산\n",
        "\n",
        "        매개변수:\n",
        "            x_in (torch.Tensor): 입력 데이터 텐서\n",
        "                x_in.shape는 (batch, dataset._max_seq_length)입니다.\n",
        "            apply_softmax (bool): 소프트맥스 활성화 함수를 위한 플래그\n",
        "                크로스-엔트로피 손실을 사용하려면 False로 지정합니다\n",
        "        반환값:\n",
        "            결과 텐서. tensor.shape은 (batch, num_classes)입니다.\n",
        "        \"\"\"\n",
        "\n",
        "        # 임베딩을 적용하고 특성과 채널 차원을 바꿉니다\n",
        "        x_embedded = self.emb(x_in).permute(0, 2, 1)\n",
        "\n",
        "        features = self.convnet(x_embedded)\n",
        "\n",
        "        # 평균 값을 계산하여 부가적인 차원을 제거합니다\n",
        "        remaining_size = features.size(dim=2)\n",
        "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n",
        "        features = F.dropout(features, p=self._dropout_p)\n",
        "\n",
        "        # MLP 분류기\n",
        "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self._dropout_p))\n",
        "        prediction_vector = self.fc2(intermediate_vector)\n",
        "\n",
        "        if apply_softmax:\n",
        "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
        "\n",
        "        return prediction_vector"
      ],
      "metadata": {
        "id": "6aF-N30U4TpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "# 사전훈련된 임베딩을 사용하는 CNN NewsClassifier의 매개변수\n",
        "args = Namespace(\n",
        "    # 날짜와 경로 정보\n",
        "    news_csv=\"data/ag_news/news_with_splits.csv\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"model_storage/ch5/document_classification\",\n",
        "    # 모델 하이퍼파라미터\n",
        "    glove_filepath='data/glove/glove.6B.100d.txt',\n",
        "    use_glove=False,\n",
        "    embedding_size=100,\n",
        "    hidden_dim=100,\n",
        "    num_channels=100,\n",
        "    # 훈련 하이퍼파라미터\n",
        "    seed=1337,\n",
        "    learning_rate=0.001,\n",
        "    dropout_p=0.1,\n",
        "    batch_size=128,\n",
        "    num_epochs=100,\n",
        "    early_stopping_criteria=5,\n",
        "    # 실행 옵션\n",
        "    cuda=True,\n",
        "    catch_keyboard_interrupt=True,\n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True\n",
        ")"
      ],
      "metadata": {
        "id": "5P2wzqR9Bude"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#훈련된 모델로 예측하기\n",
        "def predict_category(title, classifier, vectorizer, max_length):\n",
        "    \"\"\"뉴스 제목을 기반으로 카테고리를 예측합니다\n",
        "\n",
        "    매개변수:\n",
        "        title (str): 원시 제목 문자열\n",
        "        classifier (NewsClassifier): 훈련된 분류기 객체\n",
        "        vectorizer (NewsVectorizer): 해당 Vectorizer\n",
        "        max_length (int): 최대 시퀀스 길이\n",
        "            노트: CNN은 입력 텐서 크기에 민감합니다.\n",
        "                 훈련 데이터처럼 동일한 크기를 갖도록 만듭니다.\n",
        "    \"\"\"\n",
        "    title = preprocess_text(title)\n",
        "    vectorized_title = \\\n",
        "        torch.tensor(vectorizer.vectorize(title, vector_length=max_length))\n",
        "    result = classifier(vectorized_title.unsqueeze(0), apply_softmax=True)\n",
        "    probability_values, indices = result.max(dim=1)\n",
        "    predicted_category = vectorizer.category_vocab.lookup_index(indices.item())\n",
        "\n",
        "    return {'category': predicted_category,\n",
        "            'probability': probability_values.item()}"
      ],
      "metadata": {
        "id": "MvfpEWHCBuf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mflMmTw9BuiL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}